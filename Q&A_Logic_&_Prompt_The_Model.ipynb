{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNM+DpXgr+tTGtyeaw6CpH7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karthik6717/GenAI/blob/master/Q%26A_Logic_%26_Prompt_The_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjpFJE0m2hWB"
      },
      "outputs": [],
      "source": [
        "# Let's install key libraries\n",
        "print(\"Installing necessary libraries...\")\n",
        "!pip install -q transformers accelerate bitsandbytes torch pypdf gradio\n",
        "print(\"Libraries installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip uninstall -y transformers accelerate bitsandbytes\n",
        "# !rm -rf ~/.cache/huggingface/modules\n",
        "# !rm -rf ~/.cache/huggingface/hub\n"
      ],
      "metadata": {
        "id": "fSMl68E0_z1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's import these libraries\n",
        "import torch  # PyTorch, the backend for transformers\n",
        "import pypdf  # For reading PDFs\n",
        "import gradio as gr  # For building the UI\n",
        "from IPython.display import display, Markdown  # For nicer printing in notebooks\n",
        "print(\"Core libraries imported.\")"
      ],
      "metadata": {
        "id": "Ummilyuf25dZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from huggingface_hub import login, notebook_login\n",
        "print(\"Attempting Hugging Face login...\")\n",
        "\n",
        "# Use notebook_login() for an interactive prompt in Colab/Jupyter\n",
        "# This is generally preferred for notebooks.\n",
        "\n",
        "notebook_login()\n",
        "print(\"Login successful (or token already present)!\")"
      ],
      "metadata": {
        "id": "VksesznW26TP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "id": "Vod1YfG57uC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's import AutoModelForCasualLM\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "# Let's choose a small, powerful model suitable for Colab.\n",
        "# Alternatives you could try (might need login/agreement):\n",
        "# model_id = \"unsloth/gemma-3-4b-it-GGUF\"\n",
        "model_id = \"Qwen/Qwen2.5-3B-Instruct\"\n",
        "#model_id = \"microsoft/Phi-4-mini-instruct\"\n",
        "# model_id = \"unsloth/Llama-3.2-3B-Instruct\""
      ],
      "metadata": {
        "id": "UXOh0Xx87zHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's load the Tokenizer\n",
        "# The tokenizer prepares text input for the model\n",
        "# trust_remote_code=True is sometimes needed for newer models with custom code.\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code = True)\n",
        "print(\"Tokenizer loaded successfully.\")"
      ],
      "metadata": {
        "id": "ZmP0oXAu70kW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Force a complete reinstallation of transformers to resolve module caching issues\n",
        "#print(\"Uninstalling existing transformers...\")\n",
        "#!pip uninstall -y transformers\n",
        "#print(\"Reinstalling latest transformers...\")\n",
        "#!pip install transformers accelerate bitsandbytes torch\n",
        "\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "print(f\"Loading model: {model_id}\")\n",
        "print(\"This might take a few minutes, especially the first time...\")\n",
        "\n",
        "# Create BitsAndBytesConfig for 4-bit quantization\n",
        "quantization_config = BitsAndBytesConfig(load_in_4bit = True,\n",
        "                                         bnb_4bit_compute_dtype = torch.float16,  # or torch.bfloat16 if available\n",
        "                                         bnb_4bit_quant_type = \"nf4\",  # normal float 4 quantization\n",
        "                                         bnb_4bit_use_double_quant = True  # use nested quantization for more efficient memory usage\n",
        "                                         )\n",
        "\n",
        "# Load the model with the quantization config\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "                                             quantization_config = quantization_config,\n",
        "                                             device_map = \"auto\",\n",
        "                                             trust_remote_code = True)"
      ],
      "metadata": {
        "id": "pBD3Buwo8D8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's define a prompt\n",
        "prompt = \"Explain how Electric Vehicles work in a funny way!\"\n",
        "\n",
        "#prompt = \"What is the capital of France?\"\n",
        "\n",
        "# Method 1: Let's test the model and Tokenizer using the .generate() method!\n",
        "\n",
        "# Let's encode the input first and move it to the model's device\n",
        "inputs = tokenizer(prompt, return_tensors = \"pt\").to(model.device)\n",
        "\n",
        "# Then we will generate the output\n",
        "outputs = model.generate(**inputs, max_new_tokens = 1000)\n",
        "\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "# Using print() instead of print_markdown() because print_markdown() is not yet defined.\n",
        "print(response)"
      ],
      "metadata": {
        "id": "vAkdm-j2EKvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "# --- Get the PDF File ---\n",
        "pdf_url = \"https://www.arpnjournals.org/jeas/research_papers/rp_2025/jeas_0525_9600.pdf\"\n",
        "pdf_filename = \"jeas_0525_9600.pdf\"\n",
        "pdf_path = Path(pdf_filename)\n",
        "\n",
        "# Define headers to mimic a browser request\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "}\n",
        "\n",
        "# Download the file if it doesn't exist\n",
        "if not pdf_path.exists():\n",
        "    response = requests.get(pdf_url, headers=headers) # Pass the headers here\n",
        "    response.raise_for_status()  # Check for download errors\n",
        "    pdf_path.write_bytes(response.content)\n",
        "    print(f\"PDF downloaded successfully to {pdf_path}\")\n",
        "else:\n",
        "    print(f\"PDF file already exists at {pdf_path}\")\n",
        "\n",
        "\n",
        "# --- Read Text from PDF using pypdf ---\n",
        "pdf_text = \"\"\n",
        "\n",
        "print(f\"Reading text from {pdf_path}...\")\n",
        "reader = pypdf.PdfReader(pdf_path)\n",
        "num_pages = len(reader.pages)\n",
        "print(f\"PDF has {num_pages} pages.\")\n",
        "\n",
        "# Extract text from each page\n",
        "all_pages_text = []\n",
        "for i, page in enumerate(reader.pages):\n",
        "\n",
        "    page_text = page.extract_text()\n",
        "    if page_text:  # Only add if text extraction was successful\n",
        "        all_pages_text.append(page_text)\n",
        "    # print(f\"Read page {i+1}/{num_pages}\") # Uncomment for progress\n",
        "\n",
        "# Join the text from all pages\n",
        "pdf_text = \"\\n\".join(all_pages_text)\n",
        "print(f\"Successfully extracted text. Total characters: {len(pdf_text)}\")"
      ],
      "metadata": {
        "id": "6pafWNw33feA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_markdown(text):\n",
        "    \"\"\"Displays text as Markdown in Colab/Jupyter.\"\"\"\n",
        "    display(Markdown(text))"
      ],
      "metadata": {
        "id": "aKyGWllM5LZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display a small snippet of the PDF\n",
        "print(\"\\n--- Snippet of Extracted Text ---\")\n",
        "print_markdown(f\"{pdf_text[:1000]}\")"
      ],
      "metadata": {
        "id": "Oy-OPdze3uNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a limit for the context length to avoid overwhelming the model\n",
        "\n",
        "MAX_CONTEXT_CHARS = 6000\n",
        "\n",
        "def answer_question_from_pdf(document_text, question, llm_pipeline):\n",
        "    \"\"\"\n",
        "    Answers a question based on the provided document text using the loaded LLM pipeline.\n",
        "\n",
        "    Args:\n",
        "        document_text (str): The text extracted from the PDF.\n",
        "        question (str): The user's question.\n",
        "        llm_pipeline (transformers.pipeline): The initialized text-generation pipeline.\n",
        "\n",
        "    Returns:\n",
        "        str: The model's generated answer.\n",
        "    \"\"\"\n",
        "    # Truncate context if necessary\n",
        "    if len(document_text) > MAX_CONTEXT_CHARS:\n",
        "        print(f\"Warning: Document text ({len(document_text)} chars) exceeds limit ({MAX_CONTEXT_CHARS} chars). Truncating.\")\n",
        "        context = document_text[:MAX_CONTEXT_CHARS] + \"...\"\n",
        "    else:\n",
        "        context = document_text\n",
        "\n",
        "    # Let's define the Prompt Template\n",
        "    # We instruct the model to use only the provided document.\n",
        "    # Using a format the model expects (like Phi-3's chat format) can improve results.\n",
        "    # <|system|> provides context/instructions, <|user|> is the question.\n",
        "    # Note: Different models might prefer different prompt structures.\n",
        "    prompt_template = f\"\"\"<|system|>\n",
        "    You are an AI assistant. Answer the following question based *only* on the provided document text. If the answer is not found in the document, say \"The document does not contain information on this topic.\" Do not use any prior knowledge.\n",
        "\n",
        "    Document Text:\n",
        "    ---\n",
        "    {context}\n",
        "    ---\n",
        "    <|end|>\n",
        "    <|user|>\n",
        "    Question: {question}<|end|>\n",
        "    <|assistant|>\n",
        "    Answer:\"\"\" # We prompt the model to start generating the answer\n",
        "\n",
        "    print(f\"\\n--- Generating Answer for: '{question}' ---\")\n",
        "\n",
        "    # Run Inference on the chosen model\n",
        "    outputs = llm_pipeline(prompt_template,\n",
        "                           max_new_tokens = 500,  # Limit answer length\n",
        "                           do_sample = True,\n",
        "                           temperature = 0.2,   # Lower temperature for more factual Q&A\n",
        "                           top_p = 0.9)\n",
        "\n",
        "    # Let's extract the answer\n",
        "    # The output includes the full prompt template. We need the text generated *after* it.\n",
        "    full_generated_text = outputs[0]['generated_text']\n",
        "    answer_start_index = full_generated_text.find(\"Answer:\") + len(\"Answer:\")\n",
        "    raw_answer = full_generated_text[answer_start_index:].strip()\n",
        "\n",
        "    # Sometimes the model might still include parts of the prompt or trail off.\n",
        "    # Basic cleanup: Find the end-of-sequence token if possible, or just return raw.\n",
        "    # Phi-3 uses <|end|> or <|im_end|>\n",
        "    end_token = \"<|end|>\"\n",
        "    if end_token in raw_answer:\n",
        "            raw_answer = raw_answer.split(end_token)[0]\n",
        "\n",
        "    print(\"--- Generation Complete ---\")\n",
        "    return raw_answer\n"
      ],
      "metadata": {
        "id": "c7pcidjY5Vrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create the text-generation pipeline first\n",
        "from transformers import pipeline\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Let's test the function\n",
        "test_question = \"What is this document about?\"\n",
        "generated_answer = answer_question_from_pdf(pdf_text, test_question, pipe)\n",
        "\n",
        "print(\"\\nTest Question:\")\n",
        "print_markdown(f\"**Q:** {test_question}\")\n",
        "print(\"\\nGenerated Answer:\")\n",
        "print_markdown(f\"**A:** {generated_answer}\")"
      ],
      "metadata": {
        "id": "hs2pmton5mnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure we have the pdf_text\n",
        "# Configuration: Models available for selection\n",
        "# Use models known to fit in Colab free tier with 4-bit quantization\n",
        "\n",
        "available_models = {\n",
        "    \"Llama 3.2\": \"unsloth/Llama-3.2-3B-Instruct\",\n",
        "    \"Microsoft Phi-4 Mini\": \"microsoft/Phi-4-mini-instruct\",\n",
        "    \"Google Gemma 3\": \"unsloth/gemma-3-4b-it-GGUF\"\n",
        "    }"
      ],
      "metadata": {
        "id": "IIsQg5gyGdqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Global State (or use gr.State in Blocks) ---\n",
        "# To keep track of the currently loaded model/pipeline\n",
        "current_model_id = None\n",
        "current_pipeline = None\n",
        "print(f\"Models available for selection: {list(available_models.keys())}\")\n",
        "\n",
        "\n",
        "# Define a function to Load/Switch Models\n",
        "def load_llm_model(model_name):\n",
        "    \"\"\"Loads the selected LLM, unloading the previous one.\"\"\"\n",
        "    global current_model_id, current_pipeline, tokenizer, model\n",
        "\n",
        "    new_model_id = available_models.get(model_name)\n",
        "    if not new_model_id:\n",
        "        return \"Invalid model selected.\", None  # Return error message and None pipeline\n",
        "\n",
        "    if new_model_id == current_model_id and current_pipeline is not None:\n",
        "        print(f\"Model {model_name} is already loaded.\")\n",
        "        # Indicate success but don't reload\n",
        "        return f\"{model_name} already loaded.\", current_pipeline\n",
        "\n",
        "    print(f\"Switching to model: {model_name} ({new_model_id})...\")\n",
        "\n",
        "    # Unload previous model (important for memory)\n",
        "    # Clear variables and run garbage collection\n",
        "    current_pipeline = None\n",
        "    if \"model\" in locals():\n",
        "        del model\n",
        "    if \"tokenizer\" in locals():\n",
        "        del tokenizer\n",
        "    if \"pipe\" in locals():\n",
        "        del pipe\n",
        "    torch.cuda.empty_cache()  # Clear GPU memory cache\n",
        "    import gc\n",
        "\n",
        "    gc.collect()\n",
        "    print(\"Previous model unloaded (if any).\")\n",
        "\n",
        "    # --- Load the new model ---\n",
        "    loading_message = f\"Loading {model_name}...\"\n",
        "    try:\n",
        "        # Load Tokenizer\n",
        "        tokenizer = AutoTokenizer.from_pretrained(new_model_id, trust_remote_code = True)\n",
        "\n",
        "        # Load Model (Quantized)\n",
        "        model = AutoModelForCausalLM.from_pretrained(new_model_id,\n",
        "                                                     torch_dtype = \"auto\",  # \"torch.float16\", # Or bfloat16 if available\n",
        "                                                     load_in_4bit = True,\n",
        "                                                     device_map = \"auto\",\n",
        "                                                     trust_remote_code = True)\n",
        "\n",
        "        # Create Pipeline\n",
        "        loaded_pipeline = pipeline(\n",
        "            \"text-generation\", model = model, tokenizer = tokenizer, torch_dtype = \"auto\", device_map = \"auto\")\n",
        "\n",
        "        print(f\"Model {model_name} loaded successfully!\")\n",
        "        current_model_id = new_model_id\n",
        "        current_pipeline = loaded_pipeline  # Update global state\n",
        "        # Use locals() or return values with gr.State for better Gradio practice\n",
        "        return f\"{model_name} loaded successfully!\", loaded_pipeline  # Status message and the pipeline object\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model {model_name}: {e}\")\n",
        "        current_model_id = None\n",
        "        current_pipeline = None\n",
        "        return f\"Error loading {model_name}: {e}\", None  # Error message and None pipeline"
      ],
      "metadata": {
        "id": "YPowfwW5Gfbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Function to handle Q&A Submission ---\n",
        "# This function now relies on the globally managed 'current_pipeline'\n",
        "# In a more robust Gradio app, you'd pass the pipeline via gr.State\n",
        "def handle_submit(question):\n",
        "    \"\"\"Handles the user submitting a question.\"\"\"\n",
        "    if not current_pipeline:\n",
        "        return \"Error: No model is currently loaded. Please select a model.\"\n",
        "    if not pdf_text:\n",
        "        return \"Error: PDF text is not loaded. Please run Section 4.\"\n",
        "    if not question:\n",
        "        return \"Please enter a question.\"\n",
        "\n",
        "    print(f\"Handling submission for question: '{question}' using {current_model_id}\")\n",
        "    # Call the Q&A function defined in Section 5\n",
        "    answer = answer_question_from_pdf(pdf_text, question, current_pipeline)\n",
        "    return answer"
      ],
      "metadata": {
        "id": "jGeaWImAGweM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Build Gradio Interface using Blocks ---\n",
        "print(\"Building Gradio interface...\")\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\n",
        "        f\"\"\"\n",
        "    # PDF Q&A Bot Using Hugging Face Open-Source Models\n",
        "    Ask questions about the document ('{pdf_filename}' if loaded, {len(pdf_text)} chars).\n",
        "    Select an open-source LLM to answer your question.\n",
        "    **Note:** Switching models takes time as the new model needs to be downloaded and loaded into the GPU.\n",
        "    \"\"\"\n",
        "    )\n",
        "\n",
        "    # Store the pipeline in Gradio state for better practice (optional for this simple version)\n",
        "    # llm_pipeline_state = gr.State(None)\n",
        "\n",
        "    with gr.Row():\n",
        "        model_dropdown = gr.Dropdown(\n",
        "            choices=list(available_models.keys()),\n",
        "            label=\"ü§ñ Select LLM Model\",\n",
        "            value=list(available_models.keys())[0],  # Default to the first model\n",
        "        )\n",
        "        status_textbox = gr.Textbox(label=\"Model Status\", interactive=False)\n",
        "\n",
        "    question_textbox = gr.Textbox(\n",
        "        label=\"‚ùì Your Question\", lines=2, placeholder=\"Enter your question about the document here...\"\n",
        "    )\n",
        "    submit_button = gr.Button(\"Submit Question\", variant=\"primary\")\n",
        "    answer_textbox = gr.Textbox(label=\"üí° Answer\", lines=5, interactive=False)\n",
        "\n",
        "    # --- Event Handlers ---\n",
        "    # When the dropdown changes, load the selected model\n",
        "    model_dropdown.change(\n",
        "        fn = load_llm_model,\n",
        "        inputs = [model_dropdown],\n",
        "        outputs = [status_textbox],  # Update status text. Ideally also update a gr.State for the pipeline\n",
        "        # outputs=[status_textbox, llm_pipeline_state] # If using gr.State\n",
        "    )\n",
        "\n",
        "    # When the button is clicked, call the submit handler\n",
        "    submit_button.click(\n",
        "        fn = handle_submit,\n",
        "        inputs = [question_textbox],\n",
        "        outputs = [answer_textbox],\n",
        "        # inputs=[question_textbox, llm_pipeline_state], # Pass state if using it\n",
        "    )\n",
        "\n",
        "    # --- Initial Model Load ---\n",
        "    # Easier: Manually load first model *before* launching Gradio for simplicity here\n",
        "    initial_model_name = list(available_models.keys())[0]\n",
        "    print(f\"Performing initial load of default model: {initial_model_name}...\")\n",
        "    status, _ = load_llm_model(initial_model_name)\n",
        "    status_textbox.value = status  # Set initial status\n",
        "    print(\"Initial load complete.\")\n",
        "\n",
        "\n",
        "# --- Launch the Gradio App ---\n",
        "print(\"Launching Gradio demo...\")\n",
        "demo.launch(debug=True)  # debug=True provides more detailed logs\n",
        "\n"
      ],
      "metadata": {
        "id": "BQfbdbf_Gycq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}